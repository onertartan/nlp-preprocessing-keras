{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MrawafSittz"
   },
   "source": [
    "# Content\n",
    "This notebook series explain the basics of data preparation steps in NLP including tokenization using **tensorflow.keras.preprocessing.text**  module and padding using **tensorflow.keras.utils**  module. \n",
    "  \n",
    " In this notebook we will <br>\n",
    " 1\\. Create training texts (training corpus)<br>\n",
    " 2\\. Create a tokenizer <br>\n",
    " 3\\. Use **`fit_on_texts`** method: Fit the tokenizer on the texts to turn texts to sequence of tokens and index them.<br>\n",
    " 4\\. Use **`texts_to_sequences`** method on training texts (training corpus)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlPmAprzjg1h"
   },
   "source": [
    "### **1 . Create training texts (training corpus)**\n",
    "\n",
    "  In this series our corpus will be a list of sentences(strings). <br> In the projects we will see, training and test corpora will contain thousands of sentences. <br>\n",
    "  For simplification and demonstration purposes here we create a training corpus consisting of two sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1677398010460,
     "user": {
      "displayName": "öner tartan",
      "userId": "15982345773583695137"
     },
     "user_tz": -180
    },
    "id": "bJeeGLdHhEap",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts = [\"You take the blue-pill!\",\n",
    "               \"The story ends.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPaYN5Ysj2h6"
   },
   "source": [
    "### **2 . Create a tokenizer**\n",
    "\n",
    "* Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords[[1]](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/).<br>\n",
    "* However, usually tokenization term is used in the sense that it includes the later step, which is conversion of texts to integer(index) sequences.<br>\n",
    "* In the later courses, we will see that other libraries' tokenizers have methods like *tokenize* which directly give index representations(integer seqeunces) of the text. <br>\n",
    "Here, we will do this process in two steps:\n",
    "1. In the first step, by using **fit_to_texts** method of the tokenizer we will convert texts to pieces (words) and fill the **word_index** dictionary that tells us which word can be represented by which index.\n",
    "2. In the second step by using the **word_index** dictionary of the tokenizer we will obtain index representations(integer seqeunces) of the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FTxSOWlXPmU"
   },
   "source": [
    "For example let's consider the sentence **\"Faster but inattentive\"**.\n",
    "This sentence can be tokenized in three approaches <br>\n",
    "1. Word level as **faster** - **but** - **inattentive** <br>\n",
    "2. Sub-word level as **fast** - **er** - **but** - **in** - **attent** - **ive** <br>\n",
    "3. Character level as  **f** - **a** - **s** - **t** - **e** - **r** - **b** - **u** - **t** - **i** - **n** - **a** - **t** - **t** - **e** - **n** - **t** - **i** - **v** - **e** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiOL_aYWrUGn"
   },
   "source": [
    "If we don't specify tokenizer level,the default is **word level** tokenizer which we will use in this notebook. Now let's create the tokenizer.<br>\n",
    "First we have to import **Tokenizer** class from **tensorflow.keras.preprocessing.text** module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1677398012693,
     "user": {
      "displayName": "öner tartan",
      "userId": "15982345773583695137"
     },
     "user_tz": -180
    },
    "id": "057uK-HSj1xK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EV4I0BkB8nN6"
   },
   "source": [
    "tokenizer object has three dictionaries  which are currently empty:<br>\n",
    "* **word_index** will map words to indices<br>\n",
    "* **index_word** will map indices to words  (reverse mapping of word_index)<br>\n",
    "***word_counts** will map words to counts\n",
    "\n",
    "**word_index** and **index_word** are dictionaries whereas **word_counts** is of a special type of dictionary - ordered dictionary. OrderedDict can keep pairs in the order they are added.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHTpG85T9f-b"
   },
   "source": [
    "tokenizer will generate contents of these dictionaries based on the corpus.<br>\n",
    "Now let's see that these are currently empty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1677398015013,
     "user": {
      "displayName": "öner tartan",
      "userId": "15982345773583695137"
     },
     "user_tz": -180
    },
    "id": "wtU26_db9M1L",
    "outputId": "1e08f55a-f6a9-418b-eed7-5acc1f626986",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {}, OrderedDict())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index, tokenizer.index_word, tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJjfY4E9l_bV"
   },
   "source": [
    "### **3 .Use `fit_on_texts` method on training texts**\n",
    "* Now we have a tokenizer and a corpus. Then we can fit the tokenizer on the **training texts** (corpus) using  **`fit_on_texts`** method to tokenize words and give them indices.<br> In short, **`fit_on_texts`** method does the tokenization on the corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1677398017061,
     "user": {
      "displayName": "öner tartan",
      "userId": "15982345773583695137"
     },
     "user_tz": -180
    },
    "id": "77Je6kPpnLqC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtQwm2OpmDt_"
   },
   "source": [
    "* Using **`fit_on_texts`** method of the tokenizer, we filled tokenizer's three dictionaries based on token, their corresponding indices and counts for the given corpus. <br>\n",
    "Now let's check these dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qna-J3d7cs8q"
   },
   "source": [
    "#### 3.1. **word_index** <br>\n",
    "\n",
    "* word_index map words to indices. Below we can see that as a result of the tokenization, words are converted to lower case and punctuations are removed. \n",
    "Note that **The** and **the** are represented as the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1677398019693,
     "user": {
      "displayName": "öner tartan",
      "userId": "15982345773583695137"
     },
     "user_tz": -180
    },
    "id": "V2WDv7kU7hgl",
    "outputId": "81dffdc0-5719-4ea3-cb79-ab587f9e44bc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'you': 2, 'take': 3, 'blue': 4, 'pill': 5, 'story': 6, 'ends': 7}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbCzNIyF_PxP"
   },
   "source": [
    "*  Above note that words are indexed according to their frequency. Index 1 is given to the most common word **\"the\"** <br> \n",
    "* Also note that in this dictionary pairs are kept based on their values (which are indices), not the keys(words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ing9xn1Yc_Qt"
   },
   "source": [
    "#### 3.2. **index_word** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xAhkYfy_gPb"
   },
   "source": [
    "* Now let's check index_word dictionary which is reverse of the word_index dictionary.\n",
    "* Below note that in this dictionary pairs are kept based on keys, not values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1677398022554,
     "user": {
      "displayName": "öner tartan",
      "userId": "15982345773583695137"
     },
     "user_tz": -180
    },
    "id": "mjCfph20_r2l",
    "outputId": "f441edc4-6acd-4f60-c9c3-e4bc1ae45a85",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'the', 2: 'you', 3: 'take', 4: 'blue', 5: 'pill', 6: 'story', 7: 'ends'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45-_65Nj0MdJ"
   },
   "source": [
    "#### 3.3. **word_counts** <br>\n",
    "* Finally we can now check our third dictionary of the tokenizer **word_counts** which is ordered dictionary.<br>\n",
    "This is an OrderderDict, therefore words order are the same as they were met in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1677398024328,
     "user": {
      "displayName": "öner tartan",
      "userId": "15982345773583695137"
     },
     "user_tz": -180
    },
    "id": "yg4DiDnk0V_l",
    "outputId": "b509d9ed-98bc-4e20-a4c8-01c4b12f3849",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('you', 1), ('take', 1), ('the', 2), ('blue', 1), ('pill', 1), ('story', 1), ('ends', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax6cD5LIfXuJ"
   },
   "source": [
    "We can depict these steps as in the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./Images/Tokenizer_1.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd0HVq3GrH45"
   },
   "source": [
    "### **4. Use `texts_to_sequences` method on training texts**\n",
    "\n",
    "Convert texts to sequences of token indices (list of integer lists)  using `texts_to_sequences` method . The conversion from string to int is made based on word_index dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1677398064764,
     "user": {
      "displayName": "öner tartan",
      "userId": "15982345773583695137"
     },
     "user_tz": -180
    },
    "id": "qS8wKfhWrz1B",
    "outputId": "2b014389-6bb3-4b10-f4dc-3864b018ce91",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 1, 4, 5], [1, 6, 7]]\n"
     ]
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "print(train_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaLS9oTPtHfB"
   },
   "source": [
    "Let's print **sentences**, **word_index** dictionary and see how texts are converted to sequences of indices based on **word_index**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1677398066820,
     "user": {
      "displayName": "öner tartan",
      "userId": "15982345773583695137"
     },
     "user_tz": -180
    },
    "id": "vrgeu469tFcJ",
    "outputId": "4e60adb0-e3f7-436d-de23-fd14f9d661a2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'you': 2, 'take': 3, 'blue': 4, 'pill': 5, 'story': 6, 'ends': 7}\n",
      "['You take the blue-pill!', 'The story ends.']\n",
      "[[2, 3, 1, 4, 5], [1, 6, 7]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)\n",
    "print(train_texts)\n",
    "print(train_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7f6llhx8Fs1"
   },
   "source": [
    "We can summarize the steps and describe text_to_sequences method as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/Tokenizer_2.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUJiIiTzmVU_"
   },
   "source": [
    "References<br>\n",
    "[1] https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "colab": {
   "authorship_tag": "ABX9TyMMpWd8Mvp6hX0QWqrwKO/b",
   "mount_file_id": "1YO0jttRcG7EsuokAebKWId06K9FysD_A",
   "provenance": [
    {
     "file_id": "1YO0jttRcG7EsuokAebKWId06K9FysD_A",
     "timestamp": 1644342975588
    }
   ]
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.11.0 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.11-cpu-py39-ubuntu20.04-sagemaker-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
